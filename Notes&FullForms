
EPEL - Extra Package For Enterprise Linux (Ref: https://fedoraproject.org/wiki/EPEL)

Vertical scaling - Vertical scaling is increasing the capacity (storage or computing) of single machine to support our data processing needs.

Horizontal scaling - Horizontal Scaling is instead of adding or increasing capacity(storage or computing) to single machine we connect another machine 
to existing machine & operating system in existing machine can use the resources in newly added machine. In this software / OS gives you the view as single system
although there are multiple machines underline.
There are many systems / processing systems which are horizontally scalable like - Hadoop,Teradata,MongoDB,Cassendra,GreenPlum,etc.

Latency- Time to get first record. Where user clicks the button & get the response. Critical for interactive systems.

Thruput-Number of records processed per unit of time.

If your goal is to process Huge Data then your goal is to increase the thruput. Means High number of records processed per unit of time.
If your goal is to have an highly interactive system then your goal should be to minimize latency of your system.

Goal of Hadoop is to process huge data hense hadoop can be used for batch processing where a delay in processing data is acceptable.
Goal of MongoDB or Cassendra is to provide low latency to the user hense MongoDB or Cassendra is preferred in backend of websites.

Fault Tolerance- Fault is actually the failure of system / curruption of data/block / unavalilability of block due to n/w issue.
                 Fault Tolerance is the ability of systems to recover or keep system stable in case of fault.
                 Hadoop provide fault tolerance for HDFS using replication.


Replication- Data/File is splitted in block and there are multiple copies of same block kept on different machines in case of any block curruption/Node failure, etc.

Re-replication- Every data node(DN) sends an heart beat to name node(NN) after a specific interval of time. 
                So Name Node will come to know the failure of DN. Once NN will understand that DN is failed then it'll initiate re-replication based on available copies of data.
                If replication factor is 3 and two copies got currupt then NN will initiate re-replication for two copies.
                
Rack-Awareness- If NN keeps all 3 copies of block (If replication factor is 3) and if entire rack becomes unavailable then all three copies becomes unavailabel.
                Hense HDFS provide rack awareness feature. Where NN is aware of rack & it keeps two copies on one rack and one copy on DN on another rack.
                
Fault - Data Node Failure, Disk Failure, N/w Failure

Storage Policies- 

Erasure Encoding-

Name Node- Name node has file to block mapping, it has entire directory structure of file system or File system name space or FS image.
           Every read and write to HDFS starts from NN. If NN goes down then we won't be able to write or read from HDFS. 
           NN is the single point of failure for HDFS.

High Availability- 

FSImage- FSImage is the copy for entire HDFS Name Space. FS Image willbe entire HDFS directory structure in Disk as well as RAM memory of NN.
         Every change to FS Image is first goes as an edit entry to edit logs. These edit logs are created by Journal Node deamon on NN.
         So to recreate FS Image we need to have backup of edit logs.
         Since Journal node creates / writes to edits we need to have HA for Journal Node deamons. This is achived using  Quorum Journal Manager(QJM).
         
         So when we have HA in Hadoop then instead of writing edit entries to Journal Node NN write edit entries to QJM.
         Attaching image for HDFS HA
         
Secondary Name Node-
